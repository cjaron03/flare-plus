name: manual validation

on:
  workflow_dispatch:  # manual trigger only

concurrency:
  group: ${{ github.workflow }}
  cancel-in-progress: false

jobs:
  data-ingestion:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: flare_prediction_nightly
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: set up python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: initialize database
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_nightly
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          python scripts/init_db.py <<< "n"
      
      - name: run data ingestion with retry
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_nightly
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          python scripts/run_ingestion_with_retry.py
        id: ingestion
      
      - name: validate data freshness
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_nightly
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          python -c "
          from src.data.database import DatabaseManager
          from datetime import datetime, timedelta
          import sys
          
          db = DatabaseManager()
          
          # check if we have data from the last 24 hours
          query = '''
            SELECT COUNT(*) as count, MAX(time_tag) as latest
            FROM xray_flux
            WHERE time_tag >= NOW() - INTERVAL '24 hours'
          '''
          
          result = db.execute_query(query)
          if result:
              count = result[0]['count']
              latest = result[0]['latest']
              print(f'found {count} records, latest: {latest}')
              
              if count < 100:
                  print('warning: low record count in last 24 hours')
                  sys.exit(1)
          else:
              print('error: no data found')
              sys.exit(1)
          "
      
      - name: notify on failure
        if: failure()
        run: |
          echo "nightly validation failed"
          echo "ingestion status: ${{ steps.ingestion.outcome }}"
          # add alerting integration here (slack, email, pagerduty, etc)

  data-drift-check:
    runs-on: ubuntu-latest
    needs: data-ingestion
    if: always()
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: flare_prediction_nightly
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: set up python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: compute data statistics
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_nightly
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          python -c "
          from src.data.database import DatabaseManager
          import pandas as pd
          import numpy as np
          
          db = DatabaseManager()
          
          # get recent data statistics
          query = '''
            SELECT 
              DATE(time_tag) as date,
              COUNT(*) as record_count,
              AVG(flux) as avg_flux,
              STDDEV(flux) as std_flux,
              MAX(flux) as max_flux
            FROM xray_flux
            WHERE time_tag >= NOW() - INTERVAL '30 days'
            GROUP BY DATE(time_tag)
            ORDER BY date DESC
          '''
          
          results = db.execute_query(query)
          if results:
              df = pd.DataFrame(results)
              print('data statistics (last 30 days):')
              print(df.to_string())
              
              # basic drift detection - check for anomalies
              if len(df) > 7:
                  recent_avg = df.head(7)['avg_flux'].mean()
                  baseline_avg = df.tail(23)['avg_flux'].mean()
                  drift_ratio = abs(recent_avg - baseline_avg) / baseline_avg if baseline_avg > 0 else 0
                  
                  print(f'\ndrift ratio: {drift_ratio:.4f}')
                  if drift_ratio > 0.3:
                      print('warning: significant drift detected (>30%)')
          else:
              print('no data available for drift analysis')
          "
      
      - name: check feature availability
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_nightly
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          python -c "
          from src.data.database import DatabaseManager
          
          db = DatabaseManager()
          
          # check all required tables exist and have recent data
          tables = ['xray_flux', 'active_regions', 'solar_wind', 'geomagnetic_indices']
          
          for table in tables:
              try:
                  query = f'SELECT COUNT(*) as count FROM {table} WHERE time_tag >= NOW() - INTERVAL ''7 days'''
                  result = db.execute_query(query)
                  if result:
                      count = result[0]['count']
                      print(f'{table}: {count} records (last 7 days)')
              except Exception as e:
                  print(f'{table}: error - {e}')
          "
        continue-on-error: true

  extended-integration-tests:
    runs-on: ubuntu-latest
    needs: data-drift-check
    if: always()
    
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: flare_prediction_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - uses: actions/checkout@v4
      
      - name: set up python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'
          cache-dependency-path: 'requirements.txt'
      
      - name: cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: run extended tests
        env:
          DB_HOST: localhost
          DB_PORT: 5432
          DB_NAME: flare_prediction_test
          DB_USER: postgres
          DB_PASSWORD: postgres
        run: |
          # run all tests including slower integration tests
          pytest tests/ -v --cov=src --cov-report=term-missing --timeout=300
        continue-on-error: true
      
      - name: test noaa api integration
        run: |
          python -c "
          from src.data.fetchers import XRayFluxFetcher, ActiveRegionsFetcher
          from datetime import datetime, timedelta
          
          # test fetching real data from noaa
          xray_fetcher = XRayFluxFetcher()
          end_date = datetime.now()
          start_date = end_date - timedelta(days=1)
          
          print('testing xray flux fetcher...')
          xray_data = xray_fetcher.fetch(start_date, end_date)
          print(f'fetched {len(xray_data)} xray flux records')
          
          print('testing active regions fetcher...')
          ar_fetcher = ActiveRegionsFetcher()
          ar_data = ar_fetcher.fetch()
          print(f'fetched {len(ar_data)} active region records')
          "

